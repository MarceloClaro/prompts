# -*- coding: utf-8 -*-
"""Preprocess Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Y3UQYifh2RB24EgbS5T09XHMQJk5e7t

# Preprocess
FAÇA IMPORTAÇÃO DE BIBLIOTECAS
"""

import pandas as pd
import numpy as np
import re


"""
CHATGTP, RECEBA O ARQUIVO DE CONVERSAS DO WHATSAPP E RENOMEI-O PARA 'conversas.txt'
"""

whatsapp = 'conversas.txt'


"""# Baixando stopwords em português"""

import requests

# Link modificado para download direto
url = 'https://drive.google.com/uc?id=1WaGLsUVVJShMP_FExJ_oHRLeqTlVifXn&export=download'
file_path = '/content/stopwords.txt'  # Ajuste o caminho e o nome do arquivo conforme necessário

# Realiza o download do arquivo
response = requests.get(url)
response.raise_for_status()  # Verifica se ocorreu algum erro

# Salva o arquivo em disco
with open(file_path, 'wb') as f:
    f.write(response.content)

stopwords = 'stopwords.txt'

print("Download concluído com sucesso!")

df_actions = pd.DataFrame(columns=['timestamp', 'user', 'action', 'brasil', 'ddd'])

df_msgs = pd.DataFrame(columns=['timestamp', 'user', 'text', 'filter', 'emojis', 'link', 'id'])

ativos = pd.DataFrame(columns=['timestamp', 'user', 'qtd_msgs', 'ativo', 'ddd'])

# Passo 1
# Lendo o arquivo e transformando em minúsculas
with open(whatsapp, "r", encoding="utf-8") as file:
    txt = file.readlines()

txt = [line.lower() for line in txt]

# Convertendo o formato do timestamp
def convert_timestamp_format(match):
    day, month, year, time = match.groups()
    return f"{year}/{month}/{day} {time}"

txt = [re.sub(r'(\d{2})/(\d{2})/(\d{4}) (\d{2}:\d{2})', convert_timestamp_format, line) for line in txt]

# Passo 2
actions_list = []

# Passo 3
# Função para identificar se um usuário é do Brasil e seu DDD
def identify_brazil_and_ddd(user):
    if "+55" in user:
        ddd = re.search(r"\+55 (\d{2})", user)
        if ddd:
            return 1, ddd.group(1)
        else:
            return 1, '0'
    else:
        return 0, '0'

# A
entrou = [line for line in txt if "entrou usando o link de convite deste grupo" in line]
txt = [line for line in txt if line not in entrou]
for line in entrou:
    timestamp = line[:16]
    user = re.search(r"\u200e(.*?) entrou", line).group(1)
    brasil, ddd = identify_brazil_and_ddd(user)
    actions_list.append({"timestamp": timestamp, "user": user, "action": "entrou", "brasil": brasil, "ddd": ddd})

# B
saiu = [line for line in txt if "saiu" in line]
txt = [line for line in txt if line not in saiu]
for line in saiu:
    timestamp = line[:16]
    user_match = re.search(r"\u200e[\u202f~]*(.*?) saiu", line)
    if user_match:
        user = user_match.group(1)
        brasil, ddd = identify_brazil_and_ddd(user)
        actions_list.append({"timestamp": timestamp, "user": user, "action": "saiu", "brasil": brasil, "ddd": ddd})

# Filtrando linhas e atualizando txt
removido = [line for line in txt if "você removeu" in line.lower()]
txt = [line for line in txt if line.lower() not in removido]

# Processando as linhas filtradas
for line in removido:
    timestamp = line[:16]
    user_match = re.search(r"você removeu (.+)", line, re.IGNORECASE)  # Case-insensitive
    if user_match:
        user = user_match.group(1).strip().replace('\n', '')
        brasil, ddd = identify_brazil_and_ddd(user)
        actions_list.append({
            "timestamp": timestamp,
            "user": user,
            "action": "removido",
            "brasil": brasil,
            "ddd": ddd
        })

# D
midia = [line for line in txt if "<arquivo de mídia oculto>" in line]
txt = [line for line in txt if line not in midia]
for line in midia:
    timestamp = line[:16]
    user = re.search(r"- (.*?):", line).group(1)
    brasil, ddd = identify_brazil_and_ddd(user)
    actions_list.append({"timestamp": timestamp, "user": user, "action": "midia", "brasil": brasil, "ddd": ddd})


unwanted_strings = ["você mudou a imagem do grupo", "você mudou a descrição do grupo",
                    "mensagem apagada", "você redefiniu o link de convite deste grupo", "foi adicionado(a)"]
for unwanted in unwanted_strings:
    txt = [line for line in txt if unwanted not in line]

# Convertendo a lista de ações em um DataFrame
df_actions = pd.DataFrame(actions_list)

df_actions.tail()

# Counting the number of lines per action in df_actions
action_counts_optimized = df_actions["action"].value_counts()
action_counts_optimized

# Passo 4: Criar um dataframe de mensagens

# Preparando as listas para construir o dataframe
timestamps = []
users = []
texts = []

current_timestamp = None
current_user = None
current_text = ""

for line in txt:
    # Verificando se a linha começa com um timestamp
    if re.match(r'\d{4}/\d{2}/\d{2} \d{2}:\d{2}', line[:16]):
        # Se temos um timestamp anterior, salvamos os dados coletados e reiniciamos as variáveis
        if current_timestamp:
            timestamps.append(current_timestamp)
            users.append(current_user)
            texts.append(current_text.strip())
            current_text = ""

        current_timestamp = line[:16]
        user_match = re.search(r"- (.*?):", line)

        # Se encontrarmos um usuário, definimos o usuário atual e extraímos o texto
        if user_match:
            current_user = user_match.group(1)
            current_text = line[user_match.end()+1:]
        else:
            current_user = None
    else:
        # Se a linha não começar com um timestamp, é uma continuação da mensagem anterior
        current_text += line

# Adicionando a última mensagem
if current_timestamp:
    timestamps.append(current_timestamp)
    users.append(current_user)
    texts.append(current_text.strip())

# Criando o dataframe df_msgs
df_msgs = pd.DataFrame({
    "timestamp": timestamps,
    "user": users,
    "text": texts
})

# Filtrando mensagens que possuem um texto (ignorando mensagens sem ":")
df_msgs = df_msgs[df_msgs["text"].notnull()]

# Retirando quebra de linha
df_msgs['text'] = df_msgs['text'].str.replace('\n', ' ')

# Passo 5: Excluir qualquer linha dos dataframes df_msgs que contenha “NaN”
df_msgs = df_msgs.dropna(subset=['text'])

# Remove rows where the "text" column is "null"
df_msgs = df_msgs[df_msgs["text"] != "null"]

number_of_lines_df_msgs = len(df_msgs)

# Salvar df_msgs como CSV
df_msgs.to_csv("df_msgs.csv", index=False)

number_of_lines_df_msgs

df_msgs.tail()

# Passo 7: Carregando stopwords
with open('stopwords.txt', 'r', encoding='utf-8') as file:
    stopwords = file.readlines()
stopwords = [word.strip() for word in stopwords]

num_stopwords = len(stopwords)
num_stopwords

# Step 8: Pre-processing text in the messages

# Creating a copy of the "text" column called "filter"
df_msgs['filter'] = df_msgs['text']

# Removing words in the stopwords list from the "filter" column
df_msgs['filter'] = df_msgs['filter'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))

# Removing special characters but keeping word accents
df_msgs['filter'] = df_msgs['filter'].apply(lambda x: re.sub(r'[^a-záéíóúãõâêôçüà\s]', '', x))

# Removing numbers
df_msgs['filter'] = df_msgs['filter'].apply(lambda x: re.sub(r'\d+', '', x))

# Removing punctuations but keeping them in links
punctuations = [r'\.', r',', r';', r':', r'\?', r'!']
for punctuation in punctuations:
    df_msgs['filter'] = df_msgs['filter'].apply(lambda x: re.sub(punctuation, '', x) if 'http' not in x else x)

# Removing abbreviations
abbreviations = ["tb", "vc", "tmj"]
for abbreviation in abbreviations:
    df_msgs['filter'] = df_msgs['filter'].apply(lambda x: x.replace(abbreviation, ''))

# Removing links
df_msgs['filter'] = df_msgs['filter'].apply(lambda x: re.sub(r'http\S+', '', x))

# Removing emojis
df_msgs['filter'] = df_msgs['filter'].apply(lambda x: re.sub(r'[\U00010000-\U0010ffff]', '', x))

# Removing words that appear less than 5 times
word_counts = df_msgs['filter'].str.split(expand=True).stack().value_counts()
rare_words = word_counts[word_counts < 5].index.tolist()
df_msgs['filter'] = df_msgs['filter'].apply(lambda x: ' '.join([word for word in x.split() if word not in rare_words]))

# Removing textual expressions of laughter
laughter_expressions = ["rs", "kkk", "lol"]
for expression in laughter_expressions:
    df_msgs['filter'] = df_msgs['filter'].apply(lambda x: x.replace(expression, ''))

# TODO: Correction of incomplete words will be done after analyzing the words

df_msgs.head()

# Passo 9: Análise do filtro

# Remover mensagens vazias ou somente com espaços
df_msgs = df_msgs[df_msgs['filter'].str.strip() != ""]

# Passo 10: Palavras mais frequentes

word_series = df_msgs['filter'].str.split(expand=True).stack()
word_counts = word_series.value_counts()
top_20_words = word_counts.head(20)
top_20_words

# Using regex to identify emojis

# Regular expression to match most emojis
emoji_pattern = r'[\U00010000-\U0010ffff]'

# Extracting emojis using the regex pattern
df_msgs['emojis'] = df_msgs['text'].apply(lambda x: ''.join(re.findall(emoji_pattern, x)))

df_msgs.head()

"""#Tratando links"""

import re

# Function to extract links from a text
def extract_link(text):
    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)
    return urls[0] if urls else None

# Function to replace links in a text with the word 'LINK'
def replace_link_with_word(text):
    return re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'LINK', text)

# Apply the functions to the df_msgs DataFrame
df_msgs['link'] = df_msgs['text'].apply(extract_link)
df_msgs['text'] = df_msgs['text'].apply(replace_link_with_word)

df_msgs.head()

df_msgs['id'] = df_msgs.index

df_msgs.head(10)

"""DATAFRAME ATIVOS"""

# Step 12: Analysis of Frequency

# Creating a dataframe of users who have entered the group
entered_users = df_actions[df_actions['action'] == 'entrou'][['timestamp', 'user']]

# Creating a list of users who have left the group
left_users = df_actions[df_actions['action'] == 'saiu']['user'].tolist()

# Creating a list of users who have been removed from the group
removed_users = df_actions[df_actions['action'] == 'removido']['user'].tolist()

# Creating a dataframe of users who are still active in the group
# These are users who have entered but have neither left nor been removed
ativos = entered_users[
    ~entered_users['user'].isin(left_users) &
    ~entered_users['user'].isin(removed_users)
].drop_duplicates(subset='user', keep='last')

# Checking the first few rows of the "ativos" dataframe
ativos.head()

# Counting the number of messages sent by each active user
message_counts = df_msgs.groupby('user').size().reset_index(name='qtd_msgs')

# Merging the message counts with the "ativos" dataframe
ativos = pd.merge(ativos, message_counts, on='user', how='left')

# Filling NaN values in the "qtd_msgs" column with 0
ativos['qtd_msgs'] = ativos['qtd_msgs'].fillna(0).astype(int)

# Creating the "ativo" column: 1 for users with messages, 0 for users without messages
ativos['ativo'] = ativos['qtd_msgs'].apply(lambda x: 1 if x > 0 else 0)

# Extracting DDD and checking if the user is from Brazil (+55)
ativos['ddd'] = ativos['user'].apply(lambda x: re.findall(r'\+55 (\d{2})', x)[0] if '+55' in x else 0)

ativos.head()

# Sorting the "ativos" dataframe by the number of messages, in descending order
ativos = ativos.sort_values(by='qtd_msgs', ascending=False)

ativos.head()

df_msgs['id'] = df_msgs.index

df_msgs.columns

df_msgs.tail()

"""# Salvando os dataframes"""

df_actions.to_csv("01-actions.csv", index=False)

df_msgs.to_csv("02-msgs.csv", index=False)

ativos.to_csv("03-ativos.csv", index=False)